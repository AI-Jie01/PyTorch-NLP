

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchnlp.text_encoders package &mdash; PyTorch-NLP 0.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyTorch-NLP 0.0.0 documentation" href="../index.html"/>
        <link rel="up" title="torchnlp package" href="torchnlp.html"/>
        <link rel="prev" title="torchnlp.samplers package" href="torchnlp.samplers.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyTorch-NLP
          

          
          </a>

          
            
            
              <div class="version">
                0.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="torchnlp.html">torchnlp package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="torchnlp.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="torchnlp.datasets.html">torchnlp.datasets package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.count">torchnlp.datasets.count module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.dataset">torchnlp.datasets.dataset module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.reverse">torchnlp.datasets.reverse module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.simple_qa">torchnlp.datasets.simple_qa module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.zero_to_zero">torchnlp.datasets.zero_to_zero module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchnlp.metrics.html">torchnlp.metrics package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.metrics.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics.accuracy">torchnlp.metrics.accuracy module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics.bleu">torchnlp.metrics.bleu module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchnlp.nn.html">torchnlp.nn package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.nn.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn.attention">torchnlp.nn.attention module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn.lock_dropout">torchnlp.nn.lock_dropout module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchnlp.samplers.html">torchnlp.samplers package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.bucket_batch_sampler">torchnlp.samplers.bucket_batch_sampler module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.noisy_sorted_sampler">torchnlp.samplers.noisy_sorted_sampler module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.random_batch_sampler">torchnlp.samplers.random_batch_sampler module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.sorted_sampler">torchnlp.samplers.sorted_sampler module</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">torchnlp.text_encoders package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.character_encoder">torchnlp.text_encoders.character_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.delimiter_encoder">torchnlp.text_encoders.delimiter_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.identity_encoder">torchnlp.text_encoders.identity_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.moses_encoder">torchnlp.text_encoders.moses_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.reserved_tokens">torchnlp.text_encoders.reserved_tokens module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.spacy_encoder">torchnlp.text_encoders.spacy_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.static_tokenizer_encoder">torchnlp.text_encoders.static_tokenizer_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.subword_encoder">torchnlp.text_encoders.subword_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.subword_text_tokenizer">torchnlp.text_encoders.subword_text_tokenizer module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.text_encoders">torchnlp.text_encoders.text_encoders module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.treebank_encoder">torchnlp.text_encoders.treebank_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders.word_encoder">torchnlp.text_encoders.word_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-torchnlp.text_encoders">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.html#module-torchnlp.pretrained_embeddings">torchnlp.pretrained_embeddings module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.html#module-torchnlp.utils">torchnlp.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.html#module-torchnlp">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchnlp.datasets.html">torchnlp.datasets package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.count">torchnlp.datasets.count module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.dataset">torchnlp.datasets.dataset module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.reverse">torchnlp.datasets.reverse module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.simple_qa">torchnlp.datasets.simple_qa module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets.zero_to_zero">torchnlp.datasets.zero_to_zero module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.datasets.html#module-torchnlp.datasets">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchnlp.metrics.html">torchnlp.metrics package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.metrics.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics.accuracy">torchnlp.metrics.accuracy module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics.bleu">torchnlp.metrics.bleu module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.metrics.html#module-torchnlp.metrics">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchnlp.nn.html">torchnlp.nn package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.nn.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn.attention">torchnlp.nn.attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn.lock_dropout">torchnlp.nn.lock_dropout module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.nn.html#module-torchnlp.nn">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchnlp.samplers.html">torchnlp.samplers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.bucket_batch_sampler">torchnlp.samplers.bucket_batch_sampler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.noisy_sorted_sampler">torchnlp.samplers.noisy_sorted_sampler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.random_batch_sampler">torchnlp.samplers.random_batch_sampler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers.sorted_sampler">torchnlp.samplers.sorted_sampler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchnlp.samplers.html#module-torchnlp.samplers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torchnlp.text_encoders package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.character_encoder">torchnlp.text_encoders.character_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.delimiter_encoder">torchnlp.text_encoders.delimiter_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.identity_encoder">torchnlp.text_encoders.identity_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.moses_encoder">torchnlp.text_encoders.moses_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.reserved_tokens">torchnlp.text_encoders.reserved_tokens module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.spacy_encoder">torchnlp.text_encoders.spacy_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.static_tokenizer_encoder">torchnlp.text_encoders.static_tokenizer_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.subword_encoder">torchnlp.text_encoders.subword_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.subword_text_tokenizer">torchnlp.text_encoders.subword_text_tokenizer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.text_encoders">torchnlp.text_encoders.text_encoders module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.treebank_encoder">torchnlp.text_encoders.treebank_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders.word_encoder">torchnlp.text_encoders.word_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-torchnlp.text_encoders">Module contents</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTorch-NLP</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="torchnlp.html">torchnlp package</a> &raquo;</li>
        
      <li>torchnlp.text_encoders package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/source/torchnlp.text_encoders.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torchnlp-text-encoders-package">
<h1>torchnlp.text_encoders package<a class="headerlink" href="#torchnlp-text-encoders-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-torchnlp.text_encoders.character_encoder">
<span id="torchnlp-text-encoders-character-encoder-module"></span><h2>torchnlp.text_encoders.character_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.character_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.character_encoder.CharacterEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.character_encoder.</code><code class="descname">CharacterEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/character_encoder.html#CharacterEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.character_encoder.CharacterEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Encode by splitting up by character</p>
<dl class="method">
<dt id="torchnlp.text_encoders.character_encoder.CharacterEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/character_encoder.html#CharacterEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.character_encoder.CharacterEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.delimiter_encoder">
<span id="torchnlp-text-encoders-delimiter-encoder-module"></span><h2>torchnlp.text_encoders.delimiter_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.delimiter_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.delimiter_encoder.DelimiterEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.delimiter_encoder.</code><code class="descname">DelimiterEncoder</code><span class="sig-paren">(</span><em>delimiter</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/delimiter_encoder.html#DelimiterEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.delimiter_encoder.DelimiterEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Encode by splitting up by a delimiter</p>
<dl class="method">
<dt id="torchnlp.text_encoders.delimiter_encoder.DelimiterEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/delimiter_encoder.html#DelimiterEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.delimiter_encoder.DelimiterEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.identity_encoder">
<span id="torchnlp-text-encoders-identity-encoder-module"></span><h2>torchnlp.text_encoders.identity_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.identity_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.identity_encoder.IdentityEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.identity_encoder.</code><code class="descname">IdentityEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/identity_encoder.html#IdentityEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.identity_encoder.IdentityEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>No tokenization for example: ‘Hi There’ =&gt; [‘Hi There’]</p>
<dl class="method">
<dt id="torchnlp.text_encoders.identity_encoder.IdentityEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/identity_encoder.html#IdentityEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.identity_encoder.IdentityEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.moses_encoder">
<span id="torchnlp-text-encoders-moses-encoder-module"></span><h2>torchnlp.text_encoders.moses_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.moses_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.moses_encoder.MosesEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.moses_encoder.</code><code class="descname">MosesEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/moses_encoder.html#MosesEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.moses_encoder.MosesEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.moses_encoder.MosesEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/moses_encoder.html#MosesEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.moses_encoder.MosesEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.reserved_tokens">
<span id="torchnlp-text-encoders-reserved-tokens-module"></span><h2>torchnlp.text_encoders.reserved_tokens module<a class="headerlink" href="#module-torchnlp.text_encoders.reserved_tokens" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-torchnlp.text_encoders.spacy_encoder">
<span id="torchnlp-text-encoders-spacy-encoder-module"></span><h2>torchnlp.text_encoders.spacy_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.spacy_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.spacy_encoder.SpacyEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.spacy_encoder.</code><code class="descname">SpacyEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/spacy_encoder.html#SpacyEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.spacy_encoder.SpacyEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.static_tokenizer_encoder">
<span id="torchnlp-text-encoders-static-tokenizer-encoder-module"></span><h2>torchnlp.text_encoders.static_tokenizer_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.static_tokenizer_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.static_tokenizer_encoder.</code><code class="descname">StaticTokenizerEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>min_occurrences=1</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>tokenize=&lt;function StaticTokenizerEncoder.&lt;lambda&gt;&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.text_encoders.TextEncoder" title="torchnlp.text_encoders.text_encoders.TextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.text_encoders.TextEncoder</span></code></a></p>
<p>Encoder where the tokenizer is a static callable.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode text into a vector</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder.vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of tokens</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.subword_encoder">
<span id="torchnlp-text-encoders-subword-encoder-module"></span><h2>torchnlp.text_encoders.subword_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.subword_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.subword_encoder.SubwordEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.subword_encoder.</code><code class="descname">SubwordEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>target_vocab_size=None</em>, <em>min_occurrences=1</em>, <em>max_occurrences=1000.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_encoder.SubwordEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.text_encoders.TextEncoder" title="torchnlp.text_encoders.text_encoders.TextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.text_encoders.TextEncoder</span></code></a></p>
<p>Use Googles Tensor2Tensor SubwordTextTokenizer</p>
<dl class="method">
<dt id="torchnlp.text_encoders.subword_encoder.SubwordEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_encoder.SubwordEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a tensor decode it into a string</p>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.subword_encoder.SubwordEncoder.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_encoder.SubwordEncoder.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a string encode it into a tensor</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.subword_encoder.SubwordEncoder.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.subword_encoder.SubwordEncoder.vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an array of the vocab such that index matches the token in encode</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.subword_text_tokenizer">
<span id="torchnlp-text-encoders-subword-text-tokenizer-module"></span><h2>torchnlp.text_encoders.subword_text_tokenizer module<a class="headerlink" href="#module-torchnlp.text_encoders.subword_text_tokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">SubwordTextTokenizer</code><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for invertibly encoding text using a limited vocabulary.
Invertibly encodes a native string as a sequence of subtokens from a limited
vocabulary.
A SubwordTextTokenizer is built from a corpus (so it is tailored to the text in
the corpus), and stored to a file. See text_encoder_build_subword.py.
It can then be loaded and used to encode/decode any text.
Encoding has four phases:
1. Tokenize into a list of tokens.  Each token is a unicode string of either</p>
<blockquote>
<div>all alphanumeric characters or all non-alphanumeric characters.  We drop
tokens consisting of a single space that are between two alphanumeric
tokens.</div></blockquote>
<ol class="arabic simple" start="2">
<li>Escape each token.  This escapes away special and out-of-vocabulary</li>
</ol>
<blockquote>
<div>characters, and makes sure that each token ends with an underscore, and
has no other underscores.</div></blockquote>
<ol class="arabic simple" start="3">
<li>Represent each escaped token as a the concatenation of a list of subtokens</li>
</ol>
<blockquote>
<div>from the limited vocabulary.  Subtoken selection is done greedily from
beginning to end.  That is, we construct the list in order, always picking
the longest subtoken in our vocabulary that matches a prefix of the
remaining portion of the encoded token.</div></blockquote>
<ol class="arabic simple" start="4">
<li>Concatenate these lists.  This concatenation is invertible due to the</li>
</ol>
<blockquote>
<div>fact that the trailing underscores indicate when one list is finished.</div></blockquote>
<dl class="method">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_from_corpus">
<code class="descname">build_from_corpus</code><span class="sig-paren">(</span><em>*corpuses</em>, <em>min_count=1</em>, <em>num_iterations=4</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.build_from_corpus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_from_corpus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_from_token_counts">
<code class="descname">build_from_token_counts</code><span class="sig-paren">(</span><em>token_counts</em>, <em>min_count</em>, <em>num_iterations=4</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.build_from_token_counts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_from_token_counts" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a SubwordTextTokenizer based on a dictionary of word counts.
:param token_counts: a dictionary of Unicode strings to int.
:param min_count: an integer - discard subtokens with lower counts.
:param num_iterations: an integer.  how many iterations of refinement.</p>
</dd></dl>

<dl class="classmethod">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_to_target_size_from_corpus">
<em class="property">classmethod </em><code class="descname">build_to_target_size_from_corpus</code><span class="sig-paren">(</span><em>*corpuses</em>, <em>target_size=32000</em>, <em>min_val=1</em>, <em>max_val=1000.0</em>, <em>num_iterations=4</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.build_to_target_size_from_corpus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_to_target_size_from_corpus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_to_target_size_from_token_counts">
<em class="property">classmethod </em><code class="descname">build_to_target_size_from_token_counts</code><span class="sig-paren">(</span><em>target_size</em>, <em>token_counts</em>, <em>min_val</em>, <em>max_val</em>, <em>num_iterations=4</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.build_to_target_size_from_token_counts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.build_to_target_size_from_token_counts" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a SubwordTextTokenizer that has <cite>vocab_size</cite> near <cite>target_size</cite>.
Uses simple recursive binary search to find a minimum token count that most
closely matches the <cite>target_size</cite>.
:param target_size: Desired vocab_size to approximate.
:param token_counts: A dictionary of token counts, mapping string to int.
:param min_val: An integer; lower bound for the minimum token count.
:param max_val: An integer; upper bound for the minimum token count.
:param num_iterations: An integer; how many iterations of refinement.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A SubwordTextTokenizer instance.</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – If <cite>min_val</cite> is greater than <cite>max_val</cite>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>subtokens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a sequence of subtoken to a native string.
:param subtokens: a list of integers in the range [0, vocab_size)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a native string</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>raw_text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#SubwordTextTokenizer.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a native string to a list of subtoken.
:param raw_text: a native string.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a list of integers in the range [0, vocab_size)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.vocab" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.vocab_size">
<code class="descname">vocab_size</code><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.SubwordTextTokenizer.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.decode">
<code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">decode</code><span class="sig-paren">(</span><em>tokens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode a list of tokens to a unicode string.
:param tokens: a list of Unicode strings</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a unicode string</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.encode">
<code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">encode</code><span class="sig-paren">(</span><em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode a unicode string as a list of tokens.
:param text: a unicode string</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a list of tokens as Unicode strings</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.native_to_unicode">
<code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">native_to_unicode</code><span class="sig-paren">(</span><em>s</em><span class="sig-paren">)</span><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.native_to_unicode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.native_to_unicode_py2">
<code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">native_to_unicode_py2</code><span class="sig-paren">(</span><em>s</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_text_tokenizer.html#native_to_unicode_py2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.native_to_unicode_py2" title="Permalink to this definition">¶</a></dt>
<dd><p>Python 2: transform native string to Unicode.</p>
</dd></dl>

<dl class="function">
<dt id="torchnlp.text_encoders.subword_text_tokenizer.unicode_to_native">
<code class="descclassname">torchnlp.text_encoders.subword_text_tokenizer.</code><code class="descname">unicode_to_native</code><span class="sig-paren">(</span><em>s</em><span class="sig-paren">)</span><a class="headerlink" href="#torchnlp.text_encoders.subword_text_tokenizer.unicode_to_native" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.text_encoders">
<span id="torchnlp-text-encoders-text-encoders-module"></span><h2>torchnlp.text_encoders.text_encoders module<a class="headerlink" href="#module-torchnlp.text_encoders.text_encoders" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.text_encoders.TextEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.text_encoders.</code><code class="descname">TextEncoder</code><a class="reference internal" href="../_modules/torchnlp/text_encoders/text_encoders.html#TextEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.text_encoders.TextEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="torchnlp.text_encoders.text_encoders.TextEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/text_encoders.html#TextEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.text_encoders.TextEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a tensor decode it into a string</p>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.text_encoders.TextEncoder.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>string</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/text_encoders.html#TextEncoder.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.text_encoders.TextEncoder.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a string encode it into a tensor</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.text_encoders.TextEncoder.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.text_encoders.TextEncoder.vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an array of the vocab such that index matches the token in encode</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.text_encoders.TextEncoder.vocab_size">
<code class="descname">vocab_size</code><a class="headerlink" href="#torchnlp.text_encoders.text_encoders.TextEncoder.vocab_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the size of the vocab used to encode the text</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.treebank_encoder">
<span id="torchnlp-text-encoders-treebank-encoder-module"></span><h2>torchnlp.text_encoders.treebank_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.treebank_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.treebank_encoder.TreebankEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.treebank_encoder.</code><code class="descname">TreebankEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/treebank_encoder.html#TreebankEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.treebank_encoder.TreebankEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.treebank_encoder.TreebankEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/treebank_encoder.html#TreebankEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.treebank_encoder.TreebankEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders.word_encoder">
<span id="torchnlp-text-encoders-word-encoder-module"></span><h2>torchnlp.text_encoders.word_encoder module<a class="headerlink" href="#module-torchnlp.text_encoders.word_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.word_encoder.WordEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.word_encoder.</code><code class="descname">WordEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>min_occurrences=1</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>tokenize=&lt;function StaticTokenizerEncoder.&lt;lambda&gt;&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/word_encoder.html#WordEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Split a string by spaces</p>
<dl class="method">
<dt id="torchnlp.text_encoders.word_encoder.WordEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/word_encoder.html#WordEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.word_encoder.WordEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchnlp.text_encoders">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-torchnlp.text_encoders" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchnlp.text_encoders.CharacterEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">CharacterEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/character_encoder.html#CharacterEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.CharacterEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Encode by splitting up by character</p>
<dl class="method">
<dt id="torchnlp.text_encoders.CharacterEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/character_encoder.html#CharacterEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.CharacterEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.DelimiterEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">DelimiterEncoder</code><span class="sig-paren">(</span><em>delimiter</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/delimiter_encoder.html#DelimiterEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.DelimiterEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Encode by splitting up by a delimiter</p>
<dl class="method">
<dt id="torchnlp.text_encoders.DelimiterEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/delimiter_encoder.html#DelimiterEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.DelimiterEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.IdentityEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">IdentityEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/identity_encoder.html#IdentityEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.IdentityEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>No tokenization for example: ‘Hi There’ =&gt; [‘Hi There’]</p>
<dl class="method">
<dt id="torchnlp.text_encoders.IdentityEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/identity_encoder.html#IdentityEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.IdentityEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.MosesEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">MosesEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/moses_encoder.html#MosesEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.MosesEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.MosesEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/moses_encoder.html#MosesEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.MosesEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.SpacyEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">SpacyEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/spacy_encoder.html#SpacyEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.SpacyEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.StaticTokenizerEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">StaticTokenizerEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>min_occurrences=1</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>tokenize=&lt;function StaticTokenizerEncoder.&lt;lambda&gt;&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.StaticTokenizerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.text_encoders.TextEncoder" title="torchnlp.text_encoders.text_encoders.TextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.text_encoders.TextEncoder</span></code></a></p>
<p>Encoder where the tokenizer is a static callable.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.StaticTokenizerEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.StaticTokenizerEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.StaticTokenizerEncoder.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/static_tokenizer_encoder.html#StaticTokenizerEncoder.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.StaticTokenizerEncoder.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode text into a vector</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.StaticTokenizerEncoder.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.StaticTokenizerEncoder.vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of tokens</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.SubwordEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">SubwordEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>target_vocab_size=None</em>, <em>min_occurrences=1</em>, <em>max_occurrences=1000.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.SubwordEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.text_encoders.TextEncoder" title="torchnlp.text_encoders.text_encoders.TextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.text_encoders.TextEncoder</span></code></a></p>
<p>Use Googles Tensor2Tensor SubwordTextTokenizer</p>
<dl class="method">
<dt id="torchnlp.text_encoders.SubwordEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.SubwordEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a tensor decode it into a string</p>
</dd></dl>

<dl class="method">
<dt id="torchnlp.text_encoders.SubwordEncoder.encode">
<code class="descname">encode</code><span class="sig-paren">(</span><em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/subword_encoder.html#SubwordEncoder.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.SubwordEncoder.encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a string encode it into a tensor</p>
</dd></dl>

<dl class="attribute">
<dt id="torchnlp.text_encoders.SubwordEncoder.vocab">
<code class="descname">vocab</code><a class="headerlink" href="#torchnlp.text_encoders.SubwordEncoder.vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an array of the vocab such that index matches the token in encode</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.TreebankEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">TreebankEncoder</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/treebank_encoder.html#TreebankEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.TreebankEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.word_encoder.WordEncoder" title="torchnlp.text_encoders.word_encoder.WordEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.word_encoder.WordEncoder</span></code></a></p>
<p>Use Moses to encode.</p>
<dl class="method">
<dt id="torchnlp.text_encoders.TreebankEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/treebank_encoder.html#TreebankEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.TreebankEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchnlp.text_encoders.WordEncoder">
<em class="property">class </em><code class="descclassname">torchnlp.text_encoders.</code><code class="descname">WordEncoder</code><span class="sig-paren">(</span><em>sample</em>, <em>min_occurrences=1</em>, <em>append_eos=False</em>, <em>lower=True</em>, <em>tokenize=&lt;function StaticTokenizerEncoder.&lt;lambda&gt;&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/word_encoder.html#WordEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.WordEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder" title="torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchnlp.text_encoders.static_tokenizer_encoder.StaticTokenizerEncoder</span></code></a></p>
<p>Split a string by spaces</p>
<dl class="method">
<dt id="torchnlp.text_encoders.WordEncoder.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torchnlp/text_encoders/word_encoder.html#WordEncoder.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torchnlp.text_encoders.WordEncoder.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode vector into text, not guaranteed to be reversable</p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="torchnlp.samplers.html" class="btn btn-neutral" title="torchnlp.samplers package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Michael Petrochuk.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>